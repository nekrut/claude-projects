<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Galaxy History Import and LexicMap Tool Usage Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f7fa;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 8px;
        }

        h1 {
            color: #1a365d;
            font-size: 2.5em;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 3px solid #3182ce;
        }

        h2 {
            color: #2c5282;
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-left: 10px;
            border-left: 4px solid #3182ce;
        }

        h3 {
            color: #2d3748;
            font-size: 1.4em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        h4 {
            color: #4a5568;
            font-size: 1.1em;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
            line-height: 1.7;
        }

        a {
            color: #3182ce;
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: #2c5282;
            text-decoration: underline;
        }

        .toc {
            background-color: #f7fafc;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 30px;
            border: 1px solid #e2e8f0;
        }

        .toc h2 {
            margin-top: 0;
            font-size: 1.3em;
            border: none;
            padding: 0;
        }

        .toc ol {
            margin-left: 20px;
            margin-top: 15px;
        }

        .toc li {
            margin-bottom: 8px;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
            line-height: 1.6;
        }

        code {
            background-color: #f7fafc;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', 'Courier New', monospace;
            font-size: 0.9em;
            color: #e53e3e;
        }

        pre {
            background-color: #2d3748;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 20px;
            line-height: 1.5;
        }

        pre code {
            background-color: transparent;
            color: inherit;
            padding: 0;
            font-size: 0.9em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
            background-color: white;
        }

        table th {
            background-color: #3182ce;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
            border: 1px solid #2c5282;
        }

        table td {
            padding: 12px;
            border: 1px solid #e2e8f0;
        }

        table tr:nth-child(even) {
            background-color: #f7fafc;
        }

        table tr:hover {
            background-color: #edf2f7;
        }

        hr {
            border: none;
            border-top: 2px solid #e2e8f0;
            margin: 40px 0;
        }

        .note {
            background-color: #ebf8ff;
            border-left: 4px solid #3182ce;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 4px;
        }

        .warning {
            background-color: #fffaf0;
            border-left: 4px solid #f6ad55;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 4px;
        }

        strong {
            color: #2d3748;
            font-weight: 600;
        }

        .footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 2px solid #e2e8f0;
            color: #718096;
            font-size: 0.9em;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.5em;
            }

            pre {
                padding: 15px;
                font-size: 0.85em;
            }

            table {
                font-size: 0.9em;
            }

            table th, table td {
                padding: 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Galaxy History Import and LexicMap Tool Usage Guide</h1>

        <div class="toc">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#quick-start-guide">Quick Start Guide</a></li>
                <li><a href="#detailed-instructions-web-interface">Detailed Instructions - Web Interface</a></li>
                <li><a href="#programmatic-access-with-bioblend">Programmatic Access with BioBlend</a></li>
                <li><a href="#understanding-lexicmap">Understanding LexicMap</a></li>
                <li><a href="#advanced-usage">Advanced Usage</a></li>
                <li><a href="#troubleshooting">Troubleshooting</a></li>
            </ol>
        </div>

        <hr>

        <h2 id="quick-start-guide">Quick Start Guide</h2>

        <h3>Prerequisites</h3>
        <ul>
            <li>A Galaxy account at <a href="https://test.galaxyproject.org/" target="_blank">https://test.galaxyproject.org/</a></li>
            <li>Internet browser (Chrome, Firefox, Safari, or Edge)</li>
        </ul>

        <h3>5-Minute Workflow</h3>
        <ol>
            <li><strong>Create an account</strong> at <a href="https://test.galaxyproject.org/" target="_blank">https://test.galaxyproject.org/</a></li>
            <li><strong>Import the test history</strong>: <a href="https://test.galaxyproject.org/u/anton/h/test-history" target="_blank">https://test.galaxyproject.org/u/anton/h/test-history</a></li>
            <li><strong>Open LexicMap Search tool</strong>: <a href="https://test.galaxyproject.org/?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Flexicmap%2Flexicmap_search%2F0.8.0%2Bgalaxy0&version=latest" target="_blank">LexicMap Search Tool</a></li>
            <li><strong>Select "locally installed indices"</strong> and check as many indices as possible</li>
            <li><strong>Use datasets #1 and #2</strong> as test data or upload your own data</li>
            <li><strong>Execute</strong> and wait for results</li>
        </ol>

        <hr>

        <h2 id="detailed-instructions-web-interface">Detailed Instructions - Web Interface</h2>

        <h3>Step 1: Create a Galaxy Account</h3>
        <ol>
            <li>Navigate to <a href="https://test.galaxyproject.org/" target="_blank">https://test.galaxyproject.org/</a></li>
            <li>Click <strong>"Login or Register"</strong> in the top menu</li>
            <li>Select <strong>"Register"</strong></li>
            <li>Fill in:
                <ul>
                    <li>Email address</li>
                    <li>Password</li>
                    <li>Public name (username)</li>
                </ul>
            </li>
            <li>Click <strong>"Create"</strong></li>
            <li>Check your email for verification (if required)</li>
        </ol>

        <h3>Step 2: Import a Shared History</h3>
        <ol>
            <li>Click on the test history link: <a href="https://test.galaxyproject.org/u/anton/h/test-history" target="_blank">https://test.galaxyproject.org/u/anton/h/test-history</a></li>
            <li>This opens the shared history view</li>
            <li>Click the <strong>"+"</strong> (plus) icon or <strong>"Import history"</strong> button in the top right</li>
            <li>The history will be copied to your account</li>
            <li>Click <strong>"View history"</strong> to see the imported datasets</li>
        </ol>

        <p><strong>Supported file formats for LexicMap:</strong></p>
        <ul>
            <li>FASTA (.fasta, .fa, .fna)</li>
            <li>Compressed FASTA (.fasta.gz, .fa.gz)</li>
            <li>Minimum sequence length: &gt;150 bp recommended</li>
        </ul>

        <h3>Step 3: Navigate to LexicMap Search Tool</h3>

        <h4>Direct Link</h4>
        <p>Click this link: <a href="https://test.galaxyproject.org/?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Flexicmap%2Flexicmap_search%2F0.8.0%2Bgalaxy0&version=latest" target="_blank">LexicMap Search Tool</a></p>

        <h4>Via Tool Panel</h4>
        <ol>
            <li>In the left <strong>Tools</strong> panel, use the search box</li>
            <li>Type <strong>"lexicmap"</strong> or <strong>"sequence alignment"</strong></li>
            <li>Click on <strong>"LexicMap Search"</strong> when it appears</li>
            <li>The tool form will load in the center panel</li>
        </ol>

        <h3>Step 4: Configure LexicMap Search Parameters</h3>

        <h4>Basic Configuration</h4>

        <ol>
            <li><strong>Input sequences</strong> (Query sequences):
                <ul>
                    <li>Select your FASTA file from the dropdown</li>
                    <li>This should be dataset #1, #2, or your uploaded file</li>
                    <li>These are the sequences you want to search</li>
                </ul>
            </li>
            <li><strong>Reference genome database</strong>:
                <ul>
                    <li>Select <strong>"locally installed indices"</strong></li>
                    <li>Check as many indices as possible for comprehensive search</li>
                    <li>Available indices may include:
                        <ul>
                            <li>GTDB (Genome Taxonomy Database)</li>
                            <li>RefSeq prokaryotes</li>
                            <li>GenBank prokaryotes</li>
                            <li>Custom genome collections</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><strong>Output format</strong>:
                <ul>
                    <li>Default: Tabular (TSV) format</li>
                    <li>Includes all alignment statistics</li>
                </ul>
            </li>
        </ol>

        <h3>Step 5: Execute the Analysis</h3>
        <ol>
            <li>Review your parameter selections</li>
            <li>Click the blue <strong>"Execute"</strong> button at the bottom of the form</li>
            <li>The tool will appear in your history (right panel) with:
                <ul>
                    <li>Gray/waiting: Job queued</li>
                    <li>Yellow/running: Job executing</li>
                    <li>Green/done: Job completed successfully</li>
                    <li>Red/error: Job failed (check error message)</li>
                </ul>
            </li>
        </ol>

        <hr>

        <h2 id="programmatic-access-with-bioblend">Programmatic Access with BioBlend</h2>

        <p>For automation, scripting, and batch processing, use BioBlend (Python library).</p>

        <h3>Installation</h3>
        <pre><code>pip install bioblend</code></pre>

        <h3>Basic Setup</h3>
        <pre><code>from bioblend.galaxy import GalaxyInstance

# Connect to Galaxy
gi = GalaxyInstance(
    url='https://test.galaxyproject.org',
    key='YOUR_API_KEY'  # Get from User &gt; Preferences &gt; Manage API Key
)

# Test connection
print(gi.config.get_config())</code></pre>

        <h3>Get Your API Key</h3>
        <ol>
            <li>Log into <a href="https://test.galaxyproject.org" target="_blank">https://test.galaxyproject.org</a></li>
            <li>Click <strong>"User"</strong> → <strong>"Preferences"</strong></li>
            <li>Click <strong>"Manage API Key"</strong></li>
            <li>Click <strong>"Create a new key"</strong> (if no key exists)</li>
            <li>Copy the key and keep it secure</li>
        </ol>

        <h3>Import History via API</h3>

        <h4>Import from URL</h4>
        <pre><code># Import history from an exported archive URL
import_result = gi.histories.import_history(
    url='https://example.com/path/to/history.tar.gz'
)

print(f"Importing history: {import_result}")
# Returns job information - check status periodically</code></pre>

        <h4>Import from Local File</h4>
        <pre><code># Import from downloaded archive
import_result = gi.histories.import_history(
    file_path='/path/to/local/history_archive.tar.gz'
)</code></pre>

        <h4>Copy Shared History</h4>
        <pre><code># List public histories
published = gi.histories.get_published_histories()
for hist in published[:5]:
    print(f"{hist['name']}: {hist['id']}")

# Import a specific published history
imported = gi.histories.import_history(
    history_id='SHARED_HISTORY_ID'
)</code></pre>

        <h3>Create and Manage Histories</h3>
        <pre><code># Create new history
new_history = gi.histories.create_history(
    name='API LexicMap Analysis'
)
history_id = new_history['id']
print(f"Created history: {history_id}")

# List all your histories
histories = gi.histories.get_histories()
for h in histories:
    print(f"History: {h['name']} (ID: {h['id']})")

# View specific history
history_details = gi.histories.show_history(
    history_id=history_id,
    contents=True  # Include datasets
)

# Show datasets in history
datasets = gi.histories.show_matching_datasets(
    history_id=history_id
)
for ds in datasets:
    print(f"Dataset: {ds['name']} ({ds['state']})")</code></pre>

        <h3>Upload Data to History</h3>
        <pre><code># Upload local file
upload_result = gi.tools.upload_file(
    path='my_sequences.fasta',
    history_id=history_id,
    file_type='fasta'
)
dataset_id = upload_result['outputs'][0]['id']
print(f"Uploaded dataset ID: {dataset_id}")

# Upload from URL
upload_result = gi.tools.upload_from_ftp(
    path='https://example.com/data.fasta',
    history_id=history_id
)

# Upload large files via FTP
# First upload to FTP, then:
upload_result = gi.tools.upload_from_ftp(
    path='large_dataset.fasta',
    history_id=history_id
)</code></pre>

        <h3>Run LexicMap via API</h3>
        <pre><code># Find LexicMap tool
tools = gi.tools.get_tools()
lexicmap_tools = [t for t in tools if 'lexicmap' in t['name'].lower()]
print(f"Found tools: {lexicmap_tools}")

# Get tool ID
tool_id = 'toolshed.g2.bx.psu.edu/repos/iuc/lexicmap/lexicmap_search/0.8.0+galaxy0'

# Show tool details
tool_info = gi.tools.show_tool(tool_id=tool_id)

# Run LexicMap search
inputs = {
    'query': {'src': 'hda', 'id': dataset_id},  # Input dataset
    'reference': 'locally_cached',  # Use local indices
    'align_min_match_pident': 80,
    'min_qcov_per_genome': 70,
    'top_n_genomes': 10000
}

job_result = gi.tools.run_tool(
    history_id=history_id,
    tool_id=tool_id,
    tool_inputs=inputs
)

print(f"Job submitted: {job_result}")
output_dataset_id = job_result['outputs'][0]['id']</code></pre>

        <h3>Monitor Job Status</h3>
        <pre><code>import time

def wait_for_dataset(gi, dataset_id, timeout=3600, interval=10):
    """Wait for dataset to complete"""
    elapsed = 0
    while elapsed &lt; timeout:
        dataset = gi.datasets.show_dataset(dataset_id)
        state = dataset['state']

        print(f"Status: {state}")

        if state == 'ok':
            print("Job completed successfully!")
            return True
        elif state == 'error':
            print("Job failed!")
            return False

        time.sleep(interval)
        elapsed += interval

    print("Timeout reached")
    return False

# Use it
success = wait_for_dataset(gi, output_dataset_id)</code></pre>

        <h3>Download Results</h3>
        <pre><code># Download dataset
gi.datasets.download_dataset(
    dataset_id=output_dataset_id,
    file_path='./lexicmap_results.tsv',
    use_default_filename=False
)

print("Results downloaded!")

# Read and process results
import pandas as pd
df = pd.read_csv('lexicmap_results.tsv', sep='\t')
print(f"Found {len(df)} alignments")
print(df.head())</code></pre>

        <h3>Export History</h3>
        <pre><code># Export history
export_result = gi.histories.export_history(
    history_id=history_id,
    gzip=True,
    include_hidden=False,
    include_deleted=False,
    wait=True  # Wait for export to complete
)

# Download exported history
gi.histories.download_history(
    history_id=history_id,
    jeha_id=export_result['id'],
    outf='my_analysis_export.tar.gz'
)

print("History exported!")</code></pre>

        <h3>Complete Automation Script</h3>
        <pre><code>#!/usr/bin/env python3
"""
Automated LexicMap analysis pipeline
"""
import time
from bioblend.galaxy import GalaxyInstance

# Configuration
GALAXY_URL = 'https://test.galaxyproject.org'
API_KEY = 'YOUR_API_KEY'
QUERY_FILE = 'input_sequences.fasta'

# Initialize
gi = GalaxyInstance(url=GALAXY_URL, key=API_KEY)

# Create history
print("Creating history...")
history = gi.histories.create_history(name='Automated LexicMap Run')
hist_id = history['id']

# Upload data
print("Uploading query sequences...")
upload = gi.tools.upload_file(QUERY_FILE, hist_id, file_type='fasta')
query_dataset_id = upload['outputs'][0]['id']

# Wait for upload
time.sleep(5)

# Run LexicMap
print("Running LexicMap...")
tool_id = 'toolshed.g2.bx.psu.edu/repos/iuc/lexicmap/lexicmap_search/0.8.0+galaxy0'
inputs = {
    'query': {'src': 'hda', 'id': query_dataset_id},
    'reference': 'locally_cached',
    'align_min_match_pident': 80,
    'min_qcov_per_genome': 70
}
job = gi.tools.run_tool(hist_id, tool_id, inputs)
result_dataset_id = job['outputs'][0]['id']

# Wait for completion
print("Waiting for results...")
while True:
    ds = gi.datasets.show_dataset(result_dataset_id)
    if ds['state'] == 'ok':
        break
    elif ds['state'] == 'error':
        print("Job failed!")
        exit(1)
    time.sleep(10)

# Download results
print("Downloading results...")
gi.datasets.download_dataset(result_dataset_id, 'results.tsv', False)

# Export history
print("Exporting history...")
export = gi.histories.export_history(hist_id, gzip=True, wait=True)
gi.histories.download_history(hist_id, export['id'], 'analysis.tar.gz')

print(f"Complete! View history: {GALAXY_URL}/histories/view?id={hist_id}")</code></pre>

        <hr>

        <h2 id="understanding-lexicmap">Understanding LexicMap</h2>

        <h3>What is LexicMap?</h3>
        <p>LexicMap is a specialized sequence alignment tool designed for:</p>
        <ul>
            <li><strong>Querying genes, plasmids, or viral sequences</strong> against massive genome databases</li>
            <li><strong>Fast searches</strong> across millions of prokaryotic genomes</li>
            <li><strong>Memory-efficient</strong> indexing and searching</li>
            <li><strong>Comprehensive results</strong> including all hits with high sensitivity</li>
        </ul>

        <h3>When to Use LexicMap</h3>

        <p><strong>Ideal use cases:</strong></p>
        <ul>
            <li>Identifying which organisms carry a specific gene</li>
            <li>Finding plasmid distribution across species</li>
            <li>Tracing horizontal gene transfer</li>
            <li>Screening for antimicrobial resistance genes</li>
            <li>Identifying prophages in bacterial genomes</li>
            <li>Environmental metagenomics source tracking</li>
        </ul>

        <p><strong>Not suitable for:</strong></p>
        <ul>
            <li>Protein sequence searches (use BLAST or Diamond)</li>
            <li>Very short sequences &lt;100 bp (use exact matching)</li>
            <li>Eukaryotic genome searches (optimized for prokaryotes)</li>
        </ul>

        <h3>LexicMap vs Other Tools</h3>
        <table>
            <thead>
                <tr>
                    <th>Feature</th>
                    <th>LexicMap</th>
                    <th>BLAST</th>
                    <th>DIAMOND</th>
                    <th>Bowtie2</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Input type</strong></td>
                    <td>Nucleotide</td>
                    <td>Both</td>
                    <td>Protein focus</td>
                    <td>Nucleotide</td>
                </tr>
                <tr>
                    <td><strong>Database size</strong></td>
                    <td>Millions of genomes</td>
                    <td>Limited</td>
                    <td>Large</td>
                    <td>Single ref</td>
                </tr>
                <tr>
                    <td><strong>Speed</strong></td>
                    <td>Very fast</td>
                    <td>Slow</td>
                    <td>Fast (protein)</td>
                    <td>Very fast</td>
                </tr>
                <tr>
                    <td><strong>Sensitivity</strong></td>
                    <td>High</td>
                    <td>High</td>
                    <td>High</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td><strong>Use case</strong></td>
                    <td>Prokaryote screening</td>
                    <td>General</td>
                    <td>Protein search</td>
                    <td>Mapping</td>
                </tr>
            </tbody>
        </table>

        <h3>Performance Expectations</h3>

        <p><strong>Typical run times:</strong></p>
        <ul>
            <li>Single gene (500 bp) vs 1M genomes: 30 seconds - 2 minutes</li>
            <li>Plasmid (5 kb) vs 1M genomes: 5-15 minutes</li>
            <li>Multiple queries: ~1-2 min per query</li>
            <li>16S rRNA gene vs 2.34M genomes: ~15 minutes</li>
        </ul>

        <p><strong>Memory requirements:</strong></p>
        <ul>
            <li>Index building: Depends on database size (can be GBs)</li>
            <li>Searching: Relatively low (~2-8 GB)</li>
            <li>Galaxy instance: Handled by server</li>
        </ul>

        <h3>Understanding Match Quality</h3>

        <p><strong>Query Coverage (qcovGnm):</strong></p>
        <ul>
            <li>Percentage of query that aligns to genome</li>
            <li>100% = complete match</li>
            <li>50-80% = partial match (may be fragmented assembly)</li>
            <li>&lt;50% = incomplete/divergent match</li>
        </ul>

        <p><strong>Percent Identity (pident):</strong></p>
        <ul>
            <li>Base-level identity in aligned region</li>
            <li>&gt;95% = very similar (same species/strain)</li>
            <li>85-95% = related (same genus)</li>
            <li>70-85% = distant homologs</li>
            <li>&lt;70% = weak similarity</li>
        </ul>

        <p><strong>E-value:</strong></p>
        <ul>
            <li>Statistical significance (probability of random match)</li>
            <li>&lt;1e-50 = highly significant</li>
            <li>1e-10 to 1e-50 = significant</li>
            <li>&gt;1e-5 = may be random</li>
        </ul>

        <p><strong>Practical example:</strong><br>
        If searching for a resistance gene:</p>
        <ul>
            <li>qcovGnm &gt;80% + pident &gt;90% = High confidence hit</li>
            <li>qcovGnm 50-80% + pident &gt;85% = Probable hit (investigate)</li>
            <li>qcovGnm &lt;50% or pident &lt;80% = Uncertain (needs validation)</li>
        </ul>

        <hr>

        <h2 id="advanced-usage">Advanced Usage</h2>

        <h3>Batch Processing Multiple Queries</h3>

        <h4>Via Web Interface</h4>
        <ol>
            <li>Upload a multi-FASTA file with all query sequences</li>
            <li>Run LexicMap once - it processes all sequences</li>
            <li>Results table includes all queries with their matches</li>
        </ol>

        <h4>Via BioBlend (Recommended for large batches)</h4>
        <pre><code>import glob

query_files = glob.glob('queries/*.fasta')

for query_file in query_files:
    # Upload
    upload = gi.tools.upload_file(query_file, hist_id, file_type='fasta')
    dataset_id = upload['outputs'][0]['id']

    # Run LexicMap
    job = gi.tools.run_tool(hist_id, tool_id, {
        'query': {'src': 'hda', 'id': dataset_id},
        'reference': 'locally_cached',
        'align_min_match_pident': 80
    })

    print(f"Submitted: {query_file}")</code></pre>

        <h3>Filtering and Post-Processing Results</h3>
        <pre><code>import pandas as pd

# Load results
df = pd.read_csv('lexicmap_results.tsv', sep='\t')

# Filter high-quality hits
high_quality = df[
    (df['pident'] &gt; 90) &
    (df['qcovGnm'] &gt; 80) &
    (df['evalue'] &lt; 1e-50)
]

# Count genomes per query
genome_counts = df.groupby('query')['sgenome'].nunique()
print("Genomes per query:")
print(genome_counts)

# Find queries in many genomes (widespread)
widespread = genome_counts[genome_counts &gt; 100]
print(f"Widespread genes: {list(widespread.index)}")

# Export filtered results
high_quality.to_csv('filtered_hits.tsv', sep='\t', index=False)</code></pre>

        <h3>Comparing Multiple Analyses</h3>
        <pre><code># Download multiple result datasets
result_ids = ['dataset1_id', 'dataset2_id', 'dataset3_id']
dataframes = []

for i, ds_id in enumerate(result_ids):
    gi.datasets.download_dataset(ds_id, f'temp_{i}.tsv', False)
    df = pd.read_csv(f'temp_{i}.tsv', sep='\t')
    df['analysis'] = f'Analysis_{i+1}'
    dataframes.append(df)

# Combine
combined = pd.concat(dataframes)

# Compare
pivot = combined.pivot_table(
    index='query',
    columns='analysis',
    values='hits',
    aggfunc='sum'
)
print(pivot)</code></pre>

        <h3>Using Collections for Multiple Samples</h3>
        <pre><code># Upload multiple files as collection
file_paths = ['sample1.fasta', 'sample2.fasta', 'sample3.fasta']

# Upload files
dataset_ids = []
for path in file_paths:
    upload = gi.tools.upload_file(path, hist_id, file_type='fasta')
    dataset_ids.append(upload['outputs'][0]['id'])

# Create dataset collection
collection = gi.histories.create_dataset_collection(
    history_id=hist_id,
    collection_description={
        'name': 'My Samples',
        'type': 'list',
        'elements': [
            {'name': f'sample_{i}', 'src': 'hda', 'id': ds_id}
            for i, ds_id in enumerate(dataset_ids)
        ]
    }
)

# Run LexicMap on collection (processes all samples)
job = gi.tools.run_tool(hist_id, tool_id, {
    'query': {'src': 'hdca', 'id': collection['id']},
    'reference': 'locally_cached'
})</code></pre>

        <h3>Workflow Integration</h3>
        <ol>
            <li><strong>Create a workflow</strong> in Galaxy web interface:
                <ul><li>Upload data → LexicMap → Filter results → Generate report</li></ul>
            </li>
            <li><strong>Save the workflow</strong></li>
            <li><strong>Run workflow via API:</strong></li>
        </ol>
        <pre><code># Get workflows
workflows = gi.workflows.get_workflows()
workflow_id = workflows[0]['id']

# Run workflow
workflow_result = gi.workflows.invoke_workflow(
    workflow_id=workflow_id,
    inputs={
        '0': {'src': 'hda', 'id': query_dataset_id}
    },
    history_id=hist_id
)</code></pre>

        <hr>

        <h2 id="troubleshooting">Quick Reference</h2>

        <h3>Essential Links</h3>
        <pre><code>Test Galaxy Instance: https://test.galaxyproject.org/
Test History: https://test.galaxyproject.org/u/anton/h/test-history
LexicMap Tool: https://test.galaxyproject.org/?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Flexicmap%2Flexicmap_search%2F0.8.0%2Bgalaxy0
API Docs: https://test.galaxyproject.org/api/docs</code></pre>

        <h3>Common BioBlend Commands</h3>
        <pre><code># Connect
gi = GalaxyInstance(url=URL, key=API_KEY)

# Create history
hist = gi.histories.create_history(name="My Analysis")

# Upload file
upload = gi.tools.upload_file(path, hist['id'], file_type='fasta')

# Run LexicMap
job = gi.tools.run_tool(hist['id'], tool_id, inputs)

# Check status
status = gi.datasets.show_dataset(dataset_id)

# Download result
gi.datasets.download_dataset(dataset_id, 'output.tsv', False)</code></pre>

        <h3>LexicMap Parameter Quick Reference</h3>
        <table>
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Gene Search</th>
                    <th>Plasmid Search</th>
                    <th>Strict Match</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Min % identity</td>
                    <td>80</td>
                    <td>70</td>
                    <td>95</td>
                </tr>
                <tr>
                    <td>Min qcov genome</td>
                    <td>70</td>
                    <td>50</td>
                    <td>90</td>
                </tr>
                <tr>
                    <td>Min qcov HSP</td>
                    <td>70</td>
                    <td>0</td>
                    <td>80</td>
                </tr>
                <tr>
                    <td>Min match length</td>
                    <td>-</td>
                    <td>1000</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Top N genomes</td>
                    <td>10000</td>
                    <td>10000</td>
                    <td>1000</td>
                </tr>
            </tbody>
        </table>

        <hr>

        <div class="footer">
            <p><strong>Document Version:</strong> 1.0<br>
            <strong>Last Updated:</strong> November 2024<br>
            <strong>Galaxy Test Instance Version:</strong> 26.0<br>
            <strong>LexicMap Tool Version:</strong> 0.8.0+galaxy0<br>
            <strong>Compatible BioBlend Version:</strong> 1.6.0+</p>
        </div>

        <hr>

        <h2>Appendix: Example Workflows</h2>

        <h3>Example 1: Antimicrobial Resistance Gene Screening</h3>
        <pre><code>#!/usr/bin/env python3
from bioblend.galaxy import GalaxyInstance

gi = GalaxyInstance(
    url='https://test.galaxyproject.org',
    key='YOUR_API_KEY'
)

# Create history
hist = gi.histories.create_history(name='AMR Gene Screening')

# Upload resistance genes
upload = gi.tools.upload_file('amr_genes.fasta', hist['id'], file_type='fasta')
query_id = upload['outputs'][0]['id']

# Run LexicMap with high stringency
tool_id = 'toolshed.g2.bx.psu.edu/repos/iuc/lexicmap/lexicmap_search/0.8.0+galaxy0'
job = gi.tools.run_tool(hist['id'], tool_id, {
    'query': {'src': 'hda', 'id': query_id},
    'reference': 'locally_cached',
    'align_min_match_pident': 90,  # High identity for AMR
    'min_qcov_per_genome': 80
})

print(f"AMR screening job submitted: {job['id']}")</code></pre>

        <h3>Example 2: Plasmid Distribution Analysis</h3>
        <pre><code># Lower thresholds for plasmids
job = gi.tools.run_tool(hist['id'], tool_id, {
    'query': {'src': 'hda', 'id': plasmid_dataset_id},
    'reference': 'locally_cached',
    'align_min_match_pident': 70,  # Lower for plasmids
    'min_qcov_per_genome': 50,
    'min_qcov_per_hsp': 0,  # Allow fragmented matches
    'align_min_match_len': 1000  # Longer matches
})</code></pre>

        <h3>Example 3: Multi-Sample Environmental Analysis</h3>
        <pre><code># Process multiple environmental samples
samples = ['soil_sample1.fasta', 'soil_sample2.fasta', 'water_sample.fasta']

for sample in samples:
    upload = gi.tools.upload_file(sample, hist['id'], file_type='fasta')
    query_id = upload['outputs'][0]['id']

    job = gi.tools.run_tool(hist['id'], tool_id, {
        'query': {'src': 'hda', 'id': query_id},
        'reference': 'locally_cached',
        'align_min_match_pident': 75,
        'min_qcov_per_genome': 60
    })

    print(f"Sample {sample}: Job {job['id']}")</code></pre>

    </div>
</body>
</html>